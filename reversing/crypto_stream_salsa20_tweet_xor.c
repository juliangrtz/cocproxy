__int64 __fastcall crypto_stream_salsa20_tweet_xor(
        __int64 a1,
        __int64 a2,
        unsigned __int64 a3,
        unsigned int *a4,
        unsigned int *a5)
{
  __int64 v7; // x21
  __int64 v8; // x0
  __int64 v9; // x16
  __int64 v10; // x5
  __int64 v11; // x14
  __int64 v12; // x3
  __int64 v13; // x4
  __int64 v14; // x7
  __int64 v15; // x23
  __int64 v16; // x1
  __int64 v17; // x2
  __int64 v18; // x15
  __int64 v19; // x17
  __int64 v20; // x6
  __int64 v21; // x11
  int v22; // w10
  __int64 v23; // x12
  __int64 v24; // x13
  __int64 v25; // x16
  __int64 v26; // x24
  __int64 v27; // x7
  __int64 v28; // x5
  __int64 v29; // x16
  __int64 v30; // x3
  __int64 v31; // x7
  __int64 v32; // x5
  __int64 v33; // x0
  __int64 v34; // x24
  __int64 v35; // x23
  __int64 v36; // x25
  __int64 v37; // x0
  __int64 v38; // x4
  __int64 v39; // x23
  __int64 v40; // x24
  __int64 v41; // x6
  __int64 v42; // x12
  __int64 v43; // x25
  __int64 v44; // x1
  __int64 v45; // x6
  __int64 v46; // x12
  __int64 v47; // x15
  __int64 v48; // x14
  __int64 v49; // x25
  __int64 v50; // x17
  __int64 v51; // x2
  __int64 v52; // x14
  __int64 v53; // x13
  __int64 v54; // x17
  __int64 v55; // x11
  __int64 v56; // x25
  __int64 v57; // x12
  __int64 v58; // x24
  __int64 v59; // x0
  __int64 v60; // x24
  __int64 v61; // x13
  __int64 v62; // x25
  __int64 v63; // x1
  __int64 v64; // x24
  __int64 v65; // x7
  __int64 v66; // x25
  __int64 v67; // x2
  __int64 v68; // x24
  __int64 v69; // x23
  __int64 v70; // x25
  __int64 v71; // x10
  __int64 v72; // x10
  _BYTE *v73; // x11
  __int64 v74; // x12
  unsigned __int64 v75; // x13
  bool v76; // cf
  __int64 i; // x10
  unsigned __int64 v78; // x8
  _BYTE v80[64]; // [xsp+0h] [xbp-190h] BYREF
  __int128 v81; // [xsp+40h] [xbp-150h] BYREF
  __int128 v82; // [xsp+50h] [xbp-140h]
  __int128 v83; // [xsp+60h] [xbp-130h]
  __int128 v84; // [xsp+70h] [xbp-120h]
  __int128 v85; // [xsp+80h] [xbp-110h]
  __int128 v86; // [xsp+90h] [xbp-100h]
  __int128 v87; // [xsp+A0h] [xbp-F0h]
  __int128 v88; // [xsp+B0h] [xbp-E0h]
  __int128 v89; // [xsp+C0h] [xbp-D0h]
  __int128 v90; // [xsp+D0h] [xbp-C0h]
  __int128 v91; // [xsp+E0h] [xbp-B0h]
  __int128 v92; // [xsp+F0h] [xbp-A0h]
  __int128 v93; // [xsp+100h] [xbp-90h]
  __int128 v94; // [xsp+110h] [xbp-80h]
  __int128 v95; // [xsp+120h] [xbp-70h]
  __int128 v96; // [xsp+130h] [xbp-60h]
  __int64 v97; // [xsp+148h] [xbp-48h]

  v97 = *__stack_chk_guard_ptr;
  sub_1001AC7CC(&v81, a4, a5);
  while ( 1 )
  {
    v89 = v81;
    v90 = v82;
    v91 = v83;
    v92 = v84;
    v95 = v87;
    v96 = v88;
    v93 = v85;
    v94 = v86;
    v8 = *(&v81 + 1);
    v9 = v81;
    v11 = *(&v83 + 1);
    v10 = v83;
    v13 = *(&v87 + 1);
    v12 = v87;
    v15 = *(&v85 + 1);
    v14 = v85;
    v17 = *(&v82 + 1);
    v16 = v82;
    v19 = *(&v84 + 1);
    v18 = v84;
    v21 = *(&v88 + 1);
    v20 = v88;
    v22 = 8;
    v24 = *(&v86 + 1);
    v23 = v86;
    do
    {
      v25 = v10 + v9;
      v26 = ((v12 ^ v25) << 16) | ((v12 ^ v25) >> 16);
      v27 = v26 + v14;
      v28 = ((v27 ^ v10) << 12) | ((v27 ^ v10) >> 20);
      v29 = v28 + v25;
      v30 = ((v29 ^ v26) << 8) | ((v29 ^ v26) >> 24);
      v31 = v30 + v27;
      v32 = ((v31 ^ v28) << 7) | ((v31 ^ v28) >> 25);
      v33 = v11 + v8;
      v34 = ((v13 ^ v33) << 16) | ((v13 ^ v33) >> 16);
      v35 = v34 + v15;
      v36 = ((v35 ^ v11) << 12) | ((v35 ^ v11) >> 20);
      v37 = v36 + v33;
      v38 = ((v37 ^ v34) << 8) | ((v37 ^ v34) >> 24);
      v39 = v38 + v35;
      v40 = ((v39 ^ v36) << 7) | ((v39 ^ v36) >> 25);
      v41 = ((v20 ^ (v18 + v16)) << 16) | ((v20 ^ (v18 + v16)) >> 16);
      v42 = v41 + v23;
      v43 = ((v42 ^ v18) << 12) | ((v42 ^ v18) >> 20);
      v44 = v43 + v18 + v16;
      v45 = ((v44 ^ v41) << 8) | ((v44 ^ v41) >> 24);
      v46 = v45 + v42;
      v47 = ((v46 ^ v43) << 7) | ((v46 ^ v43) >> 25);
      v48 = v19 + v17;
      v49 = ((v21 ^ (v19 + v17)) << 16) | ((v21 ^ (v19 + v17)) >> 16);
      v50 = (((v49 + v24) ^ v19) << 12) | (((v49 + v24) ^ v19) >> 20);
      v51 = v50 + v48;
      v52 = (((v50 + v48) ^ v49) << 8) | (((v50 + v48) ^ v49) >> 24);
      v53 = v52 + v49 + v24;
      v54 = ((v53 ^ v50) << 7) | ((v53 ^ v50) >> 25);
      v55 = v40 + v29;
      v56 = ((v52 ^ (v40 + v29)) << 16) | ((v52 ^ (v40 + v29)) >> 16);
      v57 = v56 + v46;
      v58 = ((v57 ^ v40) << 12) | ((v57 ^ v40) >> 20);
      v9 = v58 + v55;
      v21 = (((v58 + v55) ^ v56) << 8) | (((v58 + v55) ^ v56) >> 24);
      v23 = v21 + v57;
      v11 = ((v23 ^ v58) << 7) | ((v23 ^ v58) >> 25);
      v59 = v47 + v37;
      v60 = ((v59 ^ v30) << 16) | ((v59 ^ v30) >> 16);
      v61 = v60 + v53;
      v62 = ((v61 ^ v47) << 12) | ((v61 ^ v47) >> 20);
      v8 = v62 + v59;
      v12 = ((v8 ^ v60) << 8) | ((v8 ^ v60) >> 24);
      v24 = v12 + v61;
      v18 = ((v24 ^ v62) << 7) | ((v24 ^ v62) >> 25);
      v63 = v54 + v44;
      v64 = ((v63 ^ v38) << 16) | ((v63 ^ v38) >> 16);
      v65 = v64 + v31;
      v66 = ((v65 ^ v54) << 12) | ((v65 ^ v54) >> 20);
      v16 = v66 + v63;
      v13 = ((v16 ^ v64) << 8) | ((v16 ^ v64) >> 24);
      v14 = v13 + v65;
      v19 = ((v14 ^ v66) << 7) | ((v14 ^ v66) >> 25);
      v67 = v51 + v32;
      v68 = ((v67 ^ v45) << 16) | ((v67 ^ v45) >> 16);
      v69 = v68 + v39;
      v70 = ((v69 ^ v32) << 12) | ((v69 ^ v32) >> 20);
      v17 = v70 + v67;
      v20 = ((v17 ^ v68) << 8) | ((v17 ^ v68) >> 24);
      v15 = v20 + v69;
      v10 = ((v15 ^ v70) << 7) | ((v15 ^ v70) >> 25);
      --v22;
    }
    while ( v22 );
    v71 = 0LL;
    *&v89 = v9;
    *(&v89 + 1) = v8;
    *&v91 = ((v15 ^ v70) << 7) | ((v15 ^ v70) >> 25);
    *(&v91 + 1) = v11;
    *&v95 = v12;
    *(&v95 + 1) = v13;
    *&v93 = v14;
    *(&v93 + 1) = v15;
    *&v90 = v16;
    *(&v90 + 1) = v17;
    *&v92 = v18;
    *(&v92 + 1) = v19;
    *&v96 = ((v17 ^ v68) << 8) | ((v17 ^ v68) >> 24);
    *(&v96 + 1) = v21;
    *&v94 = v23;
    *(&v94 + 1) = v24;
    do
    {
      *(&v89 + v71) = vaddq_s64(*(&v81 + v71), *(&v89 + v71));
      v71 += 16LL;
    }
    while ( v71 != 128 );
    v72 = 0LL;
    v73 = v80;
    do
    {
      v74 = 0LL;
      v75 = *(&v89 + v72);
      do
      {
        v73[v74] = v75;
        v75 >>= 8;
        ++v74;
      }
      while ( v74 != 4 );
      ++v72;
      v73 += 4;
    }
    while ( v72 != 16 );
    v76 = __CFADD__(v87, 1LL);
    *&v87 = v87 + 1;
    if ( v76 )
      ++*(&v87 + 1);
    if ( a3 < 65 )
      break;
    for ( i = 0LL; i != 64; ++i )
      *(v7 + i) = v80[i] ^ *(a2 + i);
    a3 -= 64LL;
    a2 += 64LL;
    v7 += 64LL;
  }
  if ( a3 )
  {
    v78 = 0LL;
    do
    {
      *(v7 + v78) = v80[v78] ^ *(a2 + v78);
      ++v78;
    }
    while ( v78 < a3 );
  }
  return 0LL;
}